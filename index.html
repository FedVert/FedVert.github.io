<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css" media="screen,projection">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
  <script src="https://kit.fontawesome.com/5dafecf239.js" crossorigin="anonymous"></script>
  <link rel="icon" href="./static/images/rice.png">

  <!-- <script>
      document.addEventListener('DOMContentLoaded', function() {
      var elems = document.querySelectorAll('.materialboxed');
      var instances = M.Materialbox.init(elems, options);
      });
  </script> -->
</head>
<body class="section">
    <div class="section">
        <h3 class="header center black-text text-darken-4"><b>Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat</b></h3> 
        <h5 class="header center purple-text text-darken-3">
            <a target="_blank" href="https://github.com/huerdong">Erdong Hu</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://yuxintang-ml.github.io/">Yuxin Tang</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://akyrillidis.github.io/about/">Anastasios Kyrillidis</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://www.cs.rice.edu/~cmj4/">Chris Jermaine</a><sup>1</sup>, &nbsp; &nbsp;
        </h5>
        <h6 class="header center black-text text-darken-3"><sup>1</sup>Rice University&nbsp; &nbsp; 
        </h6>
        <div class="section">
            <div class="container">
              <div class="row">
                <h6 class="col s12 m1">
                </h6>
                <h5 class="flow-text col s12 m10">
                  <div class="center">
                    <i class="ai ai-cv ai-1x"></i> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Federated_Learning_Over_Images_Vertical_Decompositions_and_Pre-Trained_Backbones_Are_ICCV_2023_paper.html">
                      <b>Paper</b>
                    </a>
                    &emsp;
                    <i class="ai ai-arxiv ai-1x"></i> <a href="https://arxiv.org/abs/2309.03237">
                      <b>Arxiv</b>
                    </a>
                    &emsp;
                    <i class="fa fa-github fa-1x"></i> <a href="">
                      <b>Code</b>
                    </a>
<!--                     <br><br> -->
                    <br>
                    </div>
                    <!-- <br>
                    <br>
                        We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: 
                        whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; 
                        how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, 
                        we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
                        -->
                  <!-- </div>  -->
                </h5>
                <div class="col s2 m2 l2"></div>
                <div class="col s8 m8 l8">
                    <br>
                    <h5 class="center"><b>Abstract</b></h5>
                    <div class="divider"></div>
                    <p>
                        We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: 
                        whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; 
                        how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, 
                        we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
                   </p>
                   <div class="divider"></div>
                   <br>
               </div>
               <div class="col s2 m2 l2"></div>
               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Overview</b></h5><br>
                    <h6> In order to improve the ability of vision-and-language models to perform localization, many methods have incorporated further finetuning with either box or segment annotations, or rely on pretrained object detectors or box proposal networks.
                      Our work instead aims to improve the localization capabilities of models trained only on image-text pairs through weak supervision.
                      But, how can we improve the ability of a model to localize objects without access to object location annotations? 
                      Consider the example in Figure below, where a model is tasked with pointing to the location of the object <em>frisbee</em> in this image. 
                      The baseline model succeeds at finding the object but is unsuccessful at locating the object when prompted with the equivalent but more generic name <em>disc</em>. 
                      Regardless of the ability for the base model to find either of these, the visual explanations for these two prompts should be the same since the query refers to the very same object in both cases. 
                      Our work exploits this property by first generating paraphrases using a large language model and then proposing a weakly-supervised <b>Self</b>-consistency <b>EQ</b>uivalence Tuning (SelfEQ) objective 
                      that encourages consistent visual explanations between paraphrased input text pairs that refer to the same object or region in a given image.
                        <br><br>
                      <p class="center">
                      <img class="teaser" src="teaser.png" width="80%">
                      </p>
                </div>

              <div class="col s2 m2 l2"></div>
               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Method</b></h5><br>
                    <h6> Given a base pre-trained vision-and-language model purely trained on image-text pairs such as ALBEF, 
                      SelfEQ tunes the model so that for a given input image and text pair, 
                      the visual attention map extracted using GradCAM produces a similar visual attention map when provided with the same image and a text paraphrase. 
                      The figure below provides an overview of our method. 
                      Another contribution of our work consists in exploiting a large language model (LLM) to automatically generate paraphrases for existing datasets 
                      such as Visual Genome that contains textual descriptions of individual objects and regions, or MS-COCO and CC3M that contain global image descriptions. 
                      We find that SelfEQ not only expands the vocabulary of objects that the base model is able to localize but more importantly, improves the visual grounding capabilities of the model. 
                    </h6>
                        <br><br>
                    <p class="center">
                        <img class="MethodDiagram" src="MethodDiagram.png" width="80%">
                    </p>
                </div>
                <div class="col s2 m2 l2"></div>
                <div class="col s8 m8 l8">
                    <h6>Overview of our proposed weakly-supervised <b>Self</b>-consistency <b>EQ</b>uivalence tuning objective.
                         We input image-text and image-paraphrase pairs &langle;V, T&rangle; and &langle;V, T<sup>e</sup>&rangle; to our base pre-trained vision-and-language model.
                         We then obtain gradient-based visual explanations &langle;G, G<sup>e</sup>&rangle; and compute a similarity loss between them.
                         We also define an overlapping region of interest mask and encourage the model to predict consistently high saliency scores within this mask for each input pair.</h6>
                </div>
                <div class="col s2 m2 l2"></div>
               </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b> Experimental Results </b></h5><br>
                    <h6>
                      Our resulting model obtains the best performance on the task of weakly-supervised visual grounding compared to most methods under this setting 
                      and is comparable to several prior works that rely on some of box supervision. 
                      Moreover, our qualitative results show that our method can handle paraphrases and a larger vocabulary without the needed to increment the training dataset significantly.
                    </h6><br>
                    <p class="center">
                        <img class="table1" src="Table1.png" width="50%">
                        <img class="table2" src="Table2.png" width="50%">
                    </p>
                </div>
                <div class="col s2 m2 l2"></div>
               </div>

              <div class="image-row">
                <p>
                  <img src="qualitative_visual_grounding_supp.png" class="image-left">
                  <img src="qualitative_self_consistency_supp.png" class="image-right">
                </p>
              </div>
              <div class="image-row">
                <p>
                  <img src="qualitative_visual_grounding_main.png" class="image-left">
                  <img src="qualitative_self_consistency_main.png" class="image-right">
                </p>
              </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s2 m2 l2"></div>
                <div class="col s12 m12 l12">
                    <br>
                    <br>
                    <br>
                    <div class="divider"></div>
                    <h6 class="center"><b>BibTeX</b></h6>
                    <blockquote style="border-left:none; padding-left:none; padding:1.5rem">
                      <pre><code>
                      @InProceedings{Hu_2023_ICCV,
                          author    = {Hu, Erdong and Tang, Yuxin and Kyrillidis, Anastasios and Jermaine, Chris},
                          title     = {Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat},
                          booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
                          month     = {October},
                          year      = {2023},
                          pages     = {19385-19396}
                      }
                      </code></pre>
                    </blockquote>
                </div>
                <div class="col s2 m2 l2"></div>
              </div>
            </div>
        </div>
    </div>

    


</body>
</html>
