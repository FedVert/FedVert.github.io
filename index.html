<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css" media="screen,projection">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
  <script src="https://kit.fontawesome.com/5dafecf239.js" crossorigin="anonymous"></script>
  <link rel="icon" href="./static/images/rice.png">

  <!-- <script>
      document.addEventListener('DOMContentLoaded', function() {
      var elems = document.querySelectorAll('.materialboxed');
      var instances = M.Materialbox.init(elems, options);
      });
  </script> -->
</head>
<body class="section">
    <div class="section">
        <h3 class="header center black-text text-darken-4"><b>Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat</b></h3> 
        <h5 class="header center purple-text text-darken-3">
            <a target="_blank" href="https://github.com/huerdong">Erdong Hu</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://yuxintang-ml.github.io/">Yuxin Tang</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://akyrillidis.github.io/about/">Anastasios Kyrillidis</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://www.cs.rice.edu/~cmj4/">Chris Jermaine</a><sup>1</sup>, &nbsp; &nbsp;
        </h5>
        <h6 class="header center black-text text-darken-3"><sup>1</sup>Rice University&nbsp; &nbsp; 
        </h6>
        <div class="section">
            <div class="container">
              <div class="row">
                <h6 class="col s12 m1">
                </h6>
                <h5 class="flow-text col s12 m10">
                  <div class="center">
                    <i class="ai ai-cv ai-1x"></i> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Federated_Learning_Over_Images_Vertical_Decompositions_and_Pre-Trained_Backbones_Are_ICCV_2023_paper.html">
                      <b>Paper</b>
                    </a>
                    &emsp;
                    <i class="ai ai-arxiv ai-1x"></i> <a href="https://arxiv.org/abs/2309.03237">
                      <b>Arxiv</b>
                    </a>
                    &emsp;
                    <i class="fa fa-github fa-1x"></i> <a href="">
                      <b>Code</b>
                    </a>
<!--                     <br><br> -->
                    <br>
                    </div>
                    <!-- <br>
                    <br>
                        We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: 
                        whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; 
                        how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, 
                        we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
                        -->
                  <!-- </div>  -->
                </h5>
                <div class="col s2 m2 l2"></div>
                <div class="col s8 m8 l8">
                    <br>
                    <h5 class="center"><b>Abstract</b></h5>
                    <div class="divider"></div>
                    <p>
                        We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: 
                        whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; 
                        how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, 
                        we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
                   </p>
                   <div class="divider"></div>
                   <br>
               </div>
               <div class="col s2 m2 l2"></div>
               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Overview</b></h5><br>
                    <h6>
                      In the cross-device setting of federated learning, device resources, in particular computation and communication, are often limited making standard full training of the model infeasible. 
                      Distributed gradient descent, equivalent to centralized stochastic gradient descent, achieves high accuracy but at the cost of expensive computation and communication. Consequently, final accuracy alone does not make sense in a federated setting.  
                      A common approach is to evaluate accuracy over communication rounds; however, this approach inherently favors algorithms which performs more computation per round, which again does not make sense in a federated setting. We instead 
                      consider accuracy as a function of both communication cost (Bytes tranferred) and computation cost (Floating point operations used) to more fairly and realistically benchmark federated algorithms.

                      Moreover, a common benchmark for evaluation is CIFAR-100. This dataset includes a very diverse set of classes - whales, chairs, dinosaurs, etc. However, we are concerned few federated tasks will involve differentiating whales from chairs. 
                      We argue instead that it makes more sense to evaluate on a very broad range of datasets, in our case primarily from the fine-grained classifcation task. Additionally, retuning hyperparameters, e.g. 
                      learning rate, proximal weights, batch size, architecture, etc., is expensive in this resource-constrained environment and therefore we propose that off-shelf tuned performance is a more realistic setting.  

                      We present a set of benchmarks and experiments in the federated learning setting. First we demonstrate that pre-trained feature extractors are a necessary baseline in federated learning as fully training a model is drastically more expensive in comparison. 
                      Similarly, we find that decomposition methods such as IST achieve generally lower bytes transferred and FLOP for the same threshold accuracy across most datasets. While we do not propose any particular algorithm, our results suggest
                      a combination of pre-trained backbones and vertical decomposition (FedVert) perform well in this resource-constrained federated learning setting. 
                    </h6>
                    
                    <p class="center">
                      <img class="teaser" src="./static/images/fedvert_diagram.png" width="80%">
                    </p>
                </div>

              <div class="col s2 m2 l2"></div>
               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Experimental Setup</b></h5><br>
                    <h6> 
                      For the pre-trained backbone we concatenate the output features of two CNNs pre-trained on the ImageNet dataset, ResNet101 and DenseNet121, together. For most algorithms evaluated we 
                      train an MLP classifier layer with the pre-trained backbone features as inputs. 

                      We evaluate on a diverse and broad range of image classification datasets: <a target="_blank" href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a>, 
                      <a target="_blank" href="https://ai.stanford.edu/~jkrause/papers/fgvc13.pdf">Stanford Cars</a>, <a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html">VGGFLowers</a>, 
                      <a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/">Aircraft</a>, <a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/data/dtd/">Describable Textures</a>, 
                      <a target="_blank" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-100</a>. These datasets are sampled to clients of our FL simulation by Dirichlet distribution. 

                      For evaluation, we determine the highest convergent accuracy among algorithms and use 90% of this as the threshold accuracy. We measure the amount of FLOPs and bytes transferred needed for
                      each method to reach this threshold. 
                      <br><br>
                      More details can be found in the paper.
                    </h6>
                </div>
               </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b> Experimental Results </b></h5><br>
                    <h6>
                      Our resulting model obtains the best performance on the task of weakly-supervised visual grounding compared to most methods under this setting 
                      and is comparable to several prior works that rely on some of box supervision. 
                      Moreover, our qualitative results show that our method can handle paraphrases and a larger vocabulary without the needed to increment the training dataset significantly.
                    </h6><br>
                    <p class="center">
                        <img class="table1" src="Table1.png" width="50%">
                        <img class="table2" src="Table2.png" width="50%">
                    </p>
                </div>
                <div class="col s2 m2 l2"></div>
               </div>

              <div class="image-row">
                <p>
                  <img src="qualitative_visual_grounding_supp.png" class="image-left">
                  <img src="qualitative_self_consistency_supp.png" class="image-right">
                </p>
              </div>
              <div class="image-row">
                <p>
                  <img src="qualitative_visual_grounding_main.png" class="image-left">
                  <img src="qualitative_self_consistency_main.png" class="image-right">
                </p>
              </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s2 m2 l2"></div>
                <div class="col s12 m12 l12">
                    <br>
                    <br>
                    <br>
                    <div class="divider"></div>
                    <h6 class="center"><b>BibTeX</b></h6>
                    <blockquote style="border-left:none; padding-left:none; padding:1.5rem">
                      <pre><code>
                      @InProceedings{Hu_2023_ICCV,
                          author    = {Hu, Erdong and Tang, Yuxin and Kyrillidis, Anastasios and Jermaine, Chris},
                          title     = {Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat},
                          booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
                          month     = {October},
                          year      = {2023},
                          pages     = {19385-19396}
                      }
                      </code></pre>
                    </blockquote>
                </div>
                <div class="col s2 m2 l2"></div>
              </div>
            </div>
        </div>
    </div>

    


</body>
</html>
